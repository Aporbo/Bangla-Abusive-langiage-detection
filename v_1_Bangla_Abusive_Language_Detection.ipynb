{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#necessary libraries\n",
        "!pip install imbalanced-learn scikit-learn seaborn --quiet"
      ],
      "metadata": {
        "id": "WP3mE0NznBcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Qjn4rSaUnEPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading dataset from file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "file_name = list(uploaded.keys())[0]\n",
        "df = pd.read_excel(file_name)\n",
        "\n",
        "#dataset structure\n",
        "print(\"Dataset Columns:\", df.columns)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "SmvnmymHnH5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keeping only 'Text' and 'class' columns\n",
        "df = df[['Text', 'class']].copy()\n",
        "\n",
        "# Handling missing values: Drop rows where 'Text' is NaN\n",
        "df.dropna(subset=['Text'], inplace=True)\n",
        "\n",
        "# Replacing numeric values in the 'class' column with corresponding labels\n",
        "class_mapping = {\n",
        "    0.0: \"moderately toxic\",\n",
        "    1.0: \"sever toxic\",\n",
        "    2.0: \"threat\",\n",
        "    3.0: \"neutral\"\n",
        "}\n",
        "df['class'] = df['class'].map(class_mapping)\n",
        "\n",
        "#cleaned dataset\n",
        "print(\"Cleaned Dataset:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "VB66mudlnKJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning function for text comments\n",
        "def process_comments(comment):\n",
        "    if not isinstance(comment, str):\n",
        "        return \"\"\n",
        "    comment = re.sub('[^\\u0980-\\u09FF]', ' ', comment)\n",
        "    comment = re.sub('[?.`*^()!\\u09E6-\\u09EF;!,&%\\'@#$><A-Za-z0-9=./\\'\\\"_]', '', comment)\n",
        "    comment = re.sub(r'(\\W)(?=\\1)', '', comment)\n",
        "    comment = re.sub(r'https?://.*[\\r\\n]*', '', comment, flags=re.MULTILINE)\n",
        "    return comment\n",
        "\n",
        "# Clean the text column\n",
        "df['Cleaned'] = df['Text'].apply(process_comments)"
      ],
      "metadata": {
        "id": "Nah2jS06nb3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating stopwords from the dataset\n",
        "all_words = []\n",
        "df['Cleaned'].dropna().apply(lambda x: all_words.extend(x.split()))\n",
        "word_counts = Counter(all_words)\n",
        "stopword_threshold = 50\n",
        "generated_stopwords = [word for word, count in word_counts.items() if count > stopword_threshold]\n",
        "\n",
        "#remove stopwords\n",
        "def stopword_removal(comment, stopwords):\n",
        "    if not isinstance(comment, str):\n",
        "        return \"\"\n",
        "    words = comment.split()\n",
        "    filtered_words = [word.strip() for word in words if word not in stopwords]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "# stopword removal\n",
        "df['Cleaned'] = df['Cleaned'].apply(lambda x: stopword_removal(x, generated_stopwords))\n"
      ],
      "metadata": {
        "id": "gI1N3rh8nmHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with empty Cleaned comments\n",
        "df['length'] = df['Cleaned'].apply(lambda x: len(x.split()))\n",
        "df = df[df['length'] > 0].reset_index(drop=True)\n",
        "\n",
        "# Encode class labels\n",
        "le = LabelEncoder()\n",
        "df['Label'] = le.fit_transform(df['class'])\n",
        "print(\"Class names:\", le.classes_)"
      ],
      "metadata": {
        "id": "IcAvx-h1nwZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF features\n",
        "def calc_gram_tfidf(reviews, ngram_range):\n",
        "    tfidf = TfidfVectorizer(ngram_range=ngram_range, use_idf=True, tokenizer=lambda x: x.split())\n",
        "    X = tfidf.fit_transform(reviews)\n",
        "    return tfidf, X\n",
        "\n",
        "# TF-IDF for unigrams and bigrams\n",
        "tfidf, features = calc_gram_tfidf(df['Cleaned'], (1, 2))"
      ],
      "metadata": {
        "id": "3yqMTFsDn0ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "features_resampled, labels_resampled = smote.fit_resample(features, df['Label'])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_resampled, labels_resampled, test_size=0.2, random_state=42, stratify=labels_resampled)"
      ],
      "metadata": {
        "id": "wLPaTvxin762"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  machine learning models\n",
        "def model_defination():\n",
        "    return {\n",
        "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=10000, class_weight='balanced'),\n",
        "        'Naive Bayes': MultinomialNB(alpha=1.0),\n",
        "        'KNN': KNeighborsClassifier(n_neighbors=5)\n",
        "    }\n",
        "\n",
        "# Evaluate models\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, predictions)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "    plt.title(f\"Confusion Matrix - {model.__class__.__name__}\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.show()\n",
        "    return {\n",
        "        'Accuracy': accuracy_score(y_test, predictions),\n",
        "        'Precision': precision_score(y_test, predictions, average='weighted'),\n",
        "        'Recall': recall_score(y_test, predictions, average='weighted'),\n",
        "        'F1 Score': f1_score(y_test, predictions, average='weighted')\n",
        "    }\n",
        "\n",
        "# Train and evaluate models\n",
        "models = model_defination()\n",
        "performance = {}\n",
        "trained_models = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"Training and evaluating model: {name}\")\n",
        "    trained_models[name] = model.fit(X_train, y_train)  # Store trained models for later use\n",
        "    performance[name] = evaluate_model(model, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "PyXDWT3ToFnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# performance metrics to a DataFrame\n",
        "performance_df = pd.DataFrame(performance).T\n",
        "performance_df = performance_df.sort_values(by='Accuracy', ascending=False)\n",
        "\n",
        "# performance results\n",
        "print(performance_df)"
      ],
      "metadata": {
        "id": "VRCDbL3eo3V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize performance\n",
        "performance_df[['Accuracy', 'Precision', 'Recall', 'F1 Score']].plot(kind='bar', figsize=(12, 8))\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PNUfkAe_qF5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#detailed classification report for best model\n",
        "best_model_name = performance_df.index[0]\n",
        "best_model = trained_models[best_model_name]\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Classification report\n",
        "print(f\"Classification Report for {best_model_name}:\")\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))"
      ],
      "metadata": {
        "id": "9n2EhOhlqK_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_predictions_and_validate(\n",
        "    model, X_test, y_test, original_df, label_encoder, num_samples=2, confidence_threshold=0.0\n",
        "):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_prob = model.predict_proba(X_test)\n",
        "    print(\"Predictions and probabilities\")\n",
        "\n",
        "\n",
        "    # Correct and incorrect predictions storage\n",
        "    correct_predictions = []\n",
        "    incorrect_predictions = []\n",
        "\n",
        "    for i, test_index in enumerate(y_test.index):\n",
        "        if test_index not in original_df.index:\n",
        "\n",
        "            continue\n",
        "\n",
        "        # Fetch data from DataFrame\n",
        "        row = original_df.loc[test_index]\n",
        "        sample_text = row.get('Text', \"N/A\")  # Default to \"N/A\" if 'Text' column is missing\n",
        "        actual_label = row.get('class', \"Unknown\")  # Default to \"Unknown\" if 'class' column is missing\n",
        "\n",
        "        # Decode predictions and confidence\n",
        "        predicted_label = label_encoder.inverse_transform([y_pred[i]])[0]\n",
        "        confidence = max(y_pred_prob[i])\n",
        "\n",
        "\n",
        "\n",
        "        # Store predictions\n",
        "        if confidence >= confidence_threshold:\n",
        "            if y_test.iloc[i] == y_pred[i]:\n",
        "                correct_predictions.append((sample_text, actual_label, predicted_label, confidence))\n",
        "            else:\n",
        "                incorrect_predictions.append((sample_text, actual_label, predicted_label, confidence))\n",
        "\n",
        "    # --- Filtered Prediction Statistics ---\n",
        "    total_correct = len(correct_predictions)\n",
        "    total_incorrect = len(incorrect_predictions)\n",
        "    accuracy = total_correct / (total_correct + total_incorrect) if (total_correct + total_incorrect) > 0 else 0\n",
        "\n",
        "    print(\"\\n--- Filtered Prediction Statistics ---\")\n",
        "    print(f\"Total Correct Predictions: {total_correct}\")\n",
        "    print(f\"Total Incorrect Predictions: {total_incorrect}\")\n",
        "\n",
        "    # --- Class-Wise Accuracy ---\n",
        "    class_accuracies = {}\n",
        "    for class_index in y_test.unique():\n",
        "        actual_indices = (y_test == class_index)\n",
        "        correct_class_predictions = sum((y_pred[actual_indices] == y_test[actual_indices]))\n",
        "        class_name = label_encoder.inverse_transform([class_index])[0]\n",
        "        class_accuracies[class_name] = correct_class_predictions / sum(actual_indices)\n",
        "\n",
        "    print(\"\\nClass-Wise Accuracy:\")\n",
        "    for class_name, acc in class_accuracies.items():\n",
        "        print(f\"  {class_name}: {acc:.2%}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # --- Sample Predictions ---\n",
        "    sampled_correct = random.sample(correct_predictions, min(len(correct_predictions), num_samples))\n",
        "    sampled_incorrect = random.sample(incorrect_predictions, min(len(incorrect_predictions), num_samples))\n",
        "\n",
        "    # --- Correct Predictions ---\n",
        "    print(\"\\n--- Correct Predictions ---\")\n",
        "    for text, actual, predicted, conf in sampled_correct:\n",
        "        print(f\"Text: {text}\\nActual Label: {actual}\\nPredicted Label: {predicted} (Confidence: {conf:.2f})\\n{'-'*50}\")\n",
        "\n",
        "    # --- Incorrect Predictions ---\n",
        "    print(\"\\n--- Incorrect Predictions ---\")\n",
        "    for text, actual, predicted, conf in sampled_incorrect:\n",
        "        print(f\"Text: {text}\\nActual Label: {actual}\\nPredicted Label: {predicted} (Confidence: {conf:.2f})\\n{'-'*50}\")\n",
        "\n",
        "best_model_name = performance_df.index[0]  # best model name based on performance\n",
        "best_model = trained_models[best_model_name]  # Fetching the corresponding trained model\n",
        "\n",
        "# Call the function\n",
        "display_predictions_and_validate(\n",
        "    model=best_model,\n",
        "    X_test=X_test,\n",
        "    y_test=y_test,\n",
        "    original_df=df,\n",
        "    label_encoder=le,\n",
        "    num_samples=1,  # Number of samples to display\n",
        "    confidence_threshold=0.6  # Confidence threshold for filtering predictions\n",
        ")\n"
      ],
      "metadata": {
        "id": "YV3rdcF1qPhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_user_input_sentence(model, original_df, le, tfidf, preprocess_pipeline):\n",
        "\n",
        "    while True:\n",
        "        # Prompt user for input\n",
        "        user_input = input(\"\\nEnter a Bengali sentence for prediction (or type 'exit' to quit): \").strip()\n",
        "\n",
        "        # Exit condition\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Exiting... Thank you!\")\n",
        "            break\n",
        "\n",
        "        if not user_input:\n",
        "            print(\"Empty input. Please provide a valid sentence.\")\n",
        "            continue\n",
        "\n",
        "        # Preprocess the input sentence\n",
        "        cleaned_sentence = preprocess_pipeline(user_input)\n",
        "\n",
        "        # Transform the sentence using the fitted TF-IDF vectorizer\n",
        "        sentence_features = tfidf.transform([cleaned_sentence])\n",
        "\n",
        "        # Predict the label\n",
        "        predicted_class = model.predict(sentence_features)\n",
        "        predicted_label = le.inverse_transform(predicted_class)[0]\n",
        "        predicted_prob = model.predict_proba(sentence_features)\n",
        "        confidence = max(predicted_prob[0])\n",
        "\n",
        "        # Display prediction\n",
        "        print(\"\\n--- Prediction Result ---\")\n",
        "        print(f\"Input Sentence: {user_input}\")\n",
        "        print(f\"Predicted Class: {predicted_label}\")\n",
        "        print(f\"Confidence Score: {confidence:.2f}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "# Preprocessing pipeline\n",
        "def preprocess_pipeline(text):\n",
        "    cleaned_text = process_comments(text)  # Your text cleaning function\n",
        "    cleaned_text = stopword_removal(cleaned_text, generated_stopwords)  # Stopword removal\n",
        "    return cleaned_text\n",
        "\n",
        "# Call the function to allow user input\n",
        "test_user_input_sentence(best_model, df, le, tfidf, preprocess_pipeline)\n"
      ],
      "metadata": {
        "id": "wBVYicSDzb6Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}